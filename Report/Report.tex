\documentclass{article}

\usepackage{lastpage}
\usepackage{fancyhdr} % Required for custom headers
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{subcaption}
\usepackage{listings} % Required for insertion of code
\usepackage{color}
\usepackage{rotating}
\usepackage{courier} % Required for the courier font
% Below are optional packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{pdfpages}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorNameOne , \hmwkAuthorNameTwo} % Top left header
\chead{\hmwkTitle} % Top center head
\rhead{\hmwkClass} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

% Define Colors for Code Snippets in Java
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=C++,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4
}

\newcommand{\cpp}{\lstinline[language=C++]}
%------------------------------------------------------------------------------------
%   DOCUMENT STRUCTURE COMMANDS
%   Skip this unless you know what you're doing
%------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
    %\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
    %\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
    %\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
    %\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems
\setcounter{homeworkProblemCounter}{0}

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Part \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
    \stepcounter{homeworkProblemCounter} % Increase counter for number of problems
    \renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
    \section{\homeworkProblemName} % Make a section in the document with the custom problem count
    \enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
    \exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
    \noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
    \renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
    \subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
    \enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
    \enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}


%=================================================================

%------------------------------------------------------------------------------------
%   NAME AND CLASS SECTION
%------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{MapReduce Implementation of Google's PageRank Algorithm} % Assignment title
\newcommand{\hmwkSubtitle}{using MPI} % Assignment Subtitle
\newcommand{\hmwkClass}{COL380} % Course/class
\newcommand{\hmwkClassTitle}{Introduction to Parallel and Distributed Computing} % Course/class title
\newcommand{\hmwkAuthorNameOne}{Vasu Jain} % Your name
\newcommand{\hmwkAuthorNameTwo}{Shreya Sharma} % Your name
\newcommand{\hmwkEntryNumberOne}{2017CS10387} % Your entry number
\newcommand{\hmwkEntryNumberTwo}{2017CS50493} % Your entry number


%------------------------------------------------------------------------------------
%   TITLE PAGE
%------------------------------------------------------------------------------------

\title{
    \textmd{\hmwkClass :\ \hmwkClassTitle}\\
    \vspace{2in}
    \textmd{\textbf{\hmwkTitle}}\\
    \textmd{\hmwkSubtitle}\\
    % \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
    \vspace{3in}
}
        
\author
{
    \textbf{\hmwkAuthorNameOne:\ \hmwkEntryNumberOne}
    \ ,
    \textbf{\hmwkAuthorNameTwo:\ \hmwkEntryNumberTwo}
}

% Insert date here if you want it to appear below your name
\date{\today} 

%------------------------------------------------------------------------------------

\begin{document}
    
    \maketitle
    \clearpage
    \tableofcontents
    \clearpage
    
    %=========================================================
    %   Data Structure and Algorithm Optimization: 
    %=========================================================
    \section{Design Philosophy, Objectives and Workflow}
    
    We approached this assignment with the following objectives and design philosophy:
    \begin{enumerate}
        \item Implement mapreduce-pagerank using mapreduce C++ library
        \item Implement our own mapreduce library with MPI by implementing  the functions needed for pagerank. 
        \item  Implement pagerank using existing mapreduce MPI library.
    In each of the above cases compare correctness of the pagrank output against the given java/python outputs.
        \item Plot graphs with x-axis as benchmark ID, and y-axis as pagerank runtime (three graphs for the three executables).
        \item Compare pagerank latencies across the three implementations and comment on the observations
    \end{enumerate}
    The time measurements were done by using the \cpp{std::chrono::high_resolution_clock} 
    % and code was compiled with -O3 Optimization flag.
    These objectives were achieved to a large extent by continuous evolution of the code-base.
    The design philosophy caused changes across the objectives in tandem. However, the general design cycle was \\
    
    Optimize Serial $\longrightarrow$ Parallelize algorithm $\longrightarrow$ Refactor Code $\longrightarrow$ Optimize Serial $\longrightarrow$ ... \\
    
    Our code and scripts can be found in the repository at:
    {\newline}
    \url{https://github.com/jainvasu631/MPI-MapReduce-PageRank}
    \clearpage
    
    %=========================================================
    %   Data Structure and Algorithm Optimization: 
    %=========================================================
    \section{Data Structures}
    \subsection{Graphs}
    Input files contain two space separated numbers (denoting 2 web-pages) in each line indicating a link from the first webpage to the second. We read and convert the input into a graph with # of nodes (N) = maximum node index in the file.
    
    Class Graph contains - 
    \begin{itemize}
        \item VertexList (type vector(int))- Protected variable denoting a list of vertices (type int).
        \item EdgeList (type vector(pair(int,int))) - Protected variable denoting list of edges (type pair(int,int)).
        \item toList (type vector(VertexList)) - Public variable denoting a list such that toList[i] is the VertexList of all the vertices which have an edge from "i".
        \item fromList (type vector(VertexList)) - Public variable denoting a list such that fromList[i] is the VertexList of all the vertices which have an edge to "i".
    \end{itemize}{}
    
    \subsection{Columns}
    Columns is a vector of Values where Values has type - "double". Two important uses of Columns are -
    \begin{itemize}
        \item PageRank
        \item Hyperlinks
    \end{itemize}
    \subsection{Initialization}
    
    
    \subsubsection{Time Complexity Analysis}
    % The 2 parts have serial time complexity:
    % \begin{enumerate}
    %     \item Initialization - $O(N^2)$ (Quadratic due to Initialization of A and B by random numbers)
    %     \item Matrix Multiplication - $O(N^2)$ (Quadratic due to the 2 nested for loops of Size N in matrix multiplication side the main loop. The third innermost loop is of constant size as M=32)
    %     \item Checking Correctness - $O(N^2)$ ( Quadratic due to iterating over all elements of two $N\times N$ \cpp{FlatMatrix})
    % \end{enumerate}
    
    \clearpage
    %=========================================================
    %   Mapreduce C++ Library
    %=========================================================
    \section{Mapreduce C++ Library}
    The MapReduce C++ Library implements a single-machine platform for programming using the the Google MapReduce idiom. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. 
    
    The developer is required to write two classes:
    \begin{itemize}
        \item MapTask - It implements a mapping function to process key/value pairs generate a set of intermediate key/value pairs 
        \item ReduceTaskim - It implements a reduce function to merges all intermediate values associated with the same intermediate key. 
    \end{itemize}
    There are three optional template parameters that can be used to modify the default implementation:
    \begin{itemize}
        \item Datasource - It implements a mechanism to feed data to the Map Tasks - on request of the MapReduce library
        \item Combine - It can be used to partially consolidate results of the Map Task before they are passed to the Reduce Tasks
        \item IntermediateStore - It handles storage, merging and sorting of intermediate results between the Map and Reduce phases
    \end{itemize}{}
    \clearpage
    
    %=========================================================
    %   Parallelization of Algorithm: 
    %=========================================================
    \section{Parallelization of Algorithm}
    % Division of work among threads by MPI is done by assigning roughly equal number of rows of \cpp{FlatMatrix} to a thread. Also consecutive chunks of rows are usually assigned to same thread i.e. large chunk size of rows is given to each thread. This means if consecutive rows are close to each other in memory. they are being worked by the same thread avoiding false conflict misses. This also aids in cache locality for a particular process.
    
    \subsection{Embarrassingly Parallel For Loop}
    
    \subsubsection{Matrix Multiplication}
    % We have parallelized the standard matrix multiplication algorithm where the Matrix B is $32 \times N$ is passed completely to all processes in the form of \cpp{FlatMatrix}. Matrix A is $32 \times N$ is passed as evenly distributed chunks i.e as SIZE-number of sub-matrices of dimensions $N/SIZE \times 32$ and sent to threads in the order of their RANK. The thread with RANK=0 is the root thread. So each thread calculates (N/SIZE) rows of the final output matrix 'C' of the overall multiplication. Since there is no data dependence in the calculation of the 'C' matrix within its own elements, they can be independently calculated by different processes and then combined together into a single \cpp{FlatMatrix} by one thread.
     
    %=========================================================
    %   Checking Correctness 
    %=========================================================
    
     \section{Checking Correctness}
        % Correctness of the output of parallelised matrix multiplication is verified by calculating the absolute difference of corresponding elememts in the resultant matrices and ensuring they are each smaller than \cpp{Tolerance=1.0e-8}.\\
        % If difference is bigger than tolerance for any element then we return "false"(Matrices are unequal) else we return "true" (Matrices are equal).
    
    \clearpage
    
    %=========================================================
    %   Differences among different versions: 
    %=========================================================
    \section{Differences among the 3 Versions of Parallel Algorithm}
    
    \subsection{In terms of Principle}
    \clearpage
    
    \subsection{In terms of Implementation}    
    \clearpage
    
    %=========================================================
    %   Restriction on 'N': 
    %=========================================================
    % \section{Handling all values of 'N' in Parallel Algorithm}
    
    % For the Blocking and Non-Blocking Communication there is no restriction on the input matrix size dimension 'N' and the number of threads in the MPI\_Communication - 'SIZE'. 
    % \begin{itemize}
    %         \item When 'N' is a multiple of 'SIZE' we simply give CHUNK\_SIZE = (N/SIZE) number of rows of matrix 'A' to each thread to compute CHUNK\_SIZE number of rows of the output matrix. 
    %         \item When 'N' is a not a multiple of 'SIZE' we give CHUNK\_EXTRA = (N\%SIZE) number of rows of matrix 'A' to the last ranked thread in addition to the CHUNK\_SIZE rows to compute (CHUNK\_SIZE + CHUNK\_EXTRA) rows of matrix 'C'.
    %     \end{itemize}    
    
    % In case of Collective communication when 'N' is a multiple of 'SIZE' we divide the work between the threads just like we do in Blocking and Non-blocking. But when 'N' is a not a multiple of 'SIZE', we compute the CHUNK\_EXTRA = (N\%SIZE) number of rows of output matrix 'C' in the process of root thread itself and parllelize the computation of all the remaining (N - CHUNK\_EXTRA) rows of 'C' just like the case when 'N' is a multiple of 'SIZE'. 
    
    % \clearpage
    
    %=========================================================
    %   Experimental Observations of Execution Time: 
    %   Explanation of the findings and trends of graph in accordance with the theoretical time complexity: 
    %=========================================================
    \section{Observations and Conclusions}
    \subsection{Execution Time, Speedup and Efficiency}
    % The execution time was measured for \cpp{NUM_THREADS=1,2,3,4,8,16} and \cpp{N=1000,2000,4000,8000}. This data is given in Figure 1.
    
    $$\text{Speed Up}=\frac{\text{Serial Execution Time }}{\text{Parallel Execution Time }}$$
    $$\text{Efficiency}=\frac{\text{SpeedUp}}{\text{Number of Threads}}$$
    
    % \subsection{Table}
    % \begin{figure}[H]
    %     \centering
    %     \includegraphics[width=\linewidth]{Images/MPI_Master.png}
    %     \caption{Execution, Speedup and Efficiency Data for Matrix Multiplication}
    %     \label{tab:1}
    % \end{figure}
    
    \subsection{Time Complexity Analysis}
    
    % \begin{figure}[H]
    %     \centering
    %     \begin{subfigure}[H]{\linewidth}
    %         \centering
    %         \includegraphics[width=0.6\linewidth]{Images/Blocking_Parallel_Execution_Time_vs_N.png}
    %         \caption{Blocking Matrix Multiplication}
    %     \end{subfigure}
    %     \begin{subfigure}[H]{\linewidth}
    %         \centering
    %         \includegraphics[width=0.6\linewidth]{Images/Non_Blocking_Parallel_Execution_Time_vs_N.png}
    %         \caption{Non Blocking Matrix Multiplication}
    %     \end{subfigure}
    %     \begin{subfigure}[H]{\linewidth}
    %         \centering
    %         \includegraphics[width=0.6\linewidth]{Images/Collective_Parallel_Execution_Time_vs_N.png}
    %         \caption{Collective Matrix Multiplication}
    %     \end{subfigure}
        
    %     \caption{Matrix Multiplication Time Complexity for Different Number of Threads}
    %     \label{fig:0}
    % \end{figure}
    
    \subsection{Observations and Explanations of MPI Graph Trends}
    % \begin{enumerate}
    %     \item We also observe that speedup is around 1.2-2.1X when going from 1 Thread to 2 Threads and 3.1-3.3X when going to 4 Threads in Case of Blocking and Collective Matrix Multiplication. 
    %     \item However beyond 4 Threads the speedup doesn't increase further and efficiency nose dives. 
    %     This is due to the Quad Core nature of the Processor used in the experiments and it doesn't provide a significant speedup when going beyond 4 threads or processes (i.e its processor count).
    %     In fact there can be a penalty to the context switching done in higher number of threads (8 and 16). 
    %     \item The efficiency hovers between 0.7-1 when 1,2 and 4 threads are used but nosedives when more threads are used.
    %     \item We also observed that Execution Time of Blocking and Collective Communication were close to each other with Blocking being around 2-10\% faster. However this gap became relatively smaller for bigger problem sizes.
    %     \item We also observed that Execution Time of Non Blocking is much slower and speedup is significantly less than and Collective Communication were close to each other with Blocking being around 2-10\% faster. This gap increase as problem size increases.
    %     \item Another observation is that at smaller problem sizes of N=1000, speedup and efficiency are lower but they increase slowly as problem sizes goes to N=8000.
    % \end{enumerate}
    
    
    %=========================================================
    %   End
    %=========================================================
    
\end{document}
